{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d63374d-d6a1-43c5-9c57-218290117b41",
   "metadata": {},
   "source": [
    "#### About \n",
    "First make a dataset of 2d static images by getting all the timelapses, and then sampling at intervals (e.g. 20 second intervals). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161a0395-5d18-42fb-bd53-581f25f66cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "from aicsimageio import AICSImage\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../../allen_data\")\n",
    "import segmentation_core as sc\n",
    "import pipeline_utils as pu\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(pu)\n",
    "import numpy as np\n",
    "import torch \n",
    "import build_allen_dataset as bad\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from skimage import measure\n",
    "from torchvision.utils import make_grid\n",
    "import psutil\n",
    "import logging\n",
    "PATH_PROJECT=Path('/pasteur/u/jmhb/confocal-2d-video-processing')\n",
    "PATH_DATA=Path('/pasteur/data/hiPSCs_January2022')\n",
    "PATH_FNAMES_TIMELAPSE=Path('/pasteur/data/hiPSCs_January2022/fname-lookup-timelapse.csv')\n",
    "PATH_FNAMES_ZSTACK=Path('/pasteur/data/hiPSCs_January2022/fname-lookup-zstack.csv')\n",
    "SEG_FOLDER = os.path.join(PATH_DATA, \"seg-framewise\", \"feb23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc47c5-0666-432c-a78c-c953e5887134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d42d45-5510-496c-a353-f982a9df1791",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32  files\n",
      "Processed:  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, /pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-oct26-D0-only/imgs-whole-seg-samples.sav\n",
      "/pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-oct26-D0-only/meta-whole-seg-samples.sav\n"
     ]
    }
   ],
   "source": [
    "DIR_OUT = \"/pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-sep15\"\n",
    "DIR_OUT = \"/pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-oct26-D0-only\"\n",
    "\n",
    "def get_imgs_from_timelapse_segmentation():\n",
    "    \"\"\"\n",
    "    Iterate through the list of segmented files, sample frames, and save those images to file. \n",
    "    \n",
    "    Important parameter choices (that are hardcoded inside the function):\n",
    "        All the capitlised names at the start of the function. \n",
    "        The definition of `ts`, which decides how frequently to sample\n",
    "    \"\"\"\n",
    "    ## filenames for the input \n",
    "    dir_out = DIR_OUT\n",
    "    f_imgs_out = os.path.join(dir_out, \"imgs-whole-seg-samples.sav\")\n",
    "    f_meta_out = os.path.join(dir_out, \"meta-whole-seg-samples.sav\")\n",
    "\n",
    "     ### filenames from the input \n",
    "    df_fnames_timelapse = utils.get_fname_lookup(PATH_FNAMES_TIMELAPSE, PATH_DATA)\n",
    "    df_fnames_timelapse['path_file_seg'] = [os.path.join(SEG_FOLDER, row.folder, row.fname) for i, row in df_fnames_timelapse.iterrows()]\n",
    "\n",
    "\n",
    "    ### column labels and empty results array\n",
    "    columns = list(df_fnames_timelapse.columns) + ['shape','pixel_sz_Z','pixel_sz_Y','pixel_sz_X', 'channel_names', \"timestep\"]\n",
    "    all_imgs, all_meta = [], []\n",
    "    print(len(df_fnames_timelapse), \" files\")\n",
    "\n",
    "    ## iterate through image filenames \n",
    "    print(\"Processed: \", end=\" \")\n",
    "    for i, (idx, row) in enumerate(df_fnames_timelapse.iterrows()):\n",
    "        print(i, end=\", \")\n",
    "\n",
    "        img_aics = AICSImage(row.path_file_seg)    \n",
    "\n",
    "        ## save relevant metadata\n",
    "        meta = [*row, img_aics.shape, *img_aics.physical_pixel_sizes,  img_aics.channel_names]\n",
    "        T,C,Z,Y,X = img_aics.shape\n",
    "        if Z!=1:\n",
    "            logging.warning(f\"Found image with Z!=1: {row.path_file_seg}\")\n",
    "            logging.warning(f\"Skipping\")\n",
    "            continue\n",
    "        img = img_aics.data \n",
    "\n",
    "        ## get the channel order idxs\n",
    "        map_ch_idx = pu.get_channel_idx_lookup(img_aics.channel_names)\n",
    "        chs = ['lyso', 'mito', 'golgi', 'peroxy', 'er','nuclei','bodipy']\n",
    "        ch_idxs = [ map_ch_idx[k] for k in chs ]\n",
    "\n",
    "        # sample the timeseries at intervals of 20, for at most 80\n",
    "        ts = np.arange(0,min(T,81),20)\n",
    "        for t in ts:\n",
    "            all_imgs.append(img[[t],ch_idxs,0][None])\n",
    "            this_meta = meta.copy() + [t]\n",
    "            all_meta.append(this_meta)\n",
    "            assert len(columns)==len(this_meta), \"code error\"\n",
    "        assert len(all_imgs)==len(all_meta)\n",
    "\n",
    "    all_imgs = torch.from_numpy(np.concatenate(all_imgs).astype(np.int8))\n",
    "    df_meta_cell = pd.DataFrame(data=all_meta, columns=columns)\n",
    "    df_meta_cell['cellid'] = df_meta_cell.g1 + \"-\"+ df_meta_cell.g2 +\"-\"+ df_meta_cell.g3.apply(str)\n",
    "    df_meta_cell['cellid_ts'] = df_meta_cell.cellid + \"-\" + df_meta_cell.timestep.astype(str)\n",
    "\n",
    "    ## save \n",
    "    print(f_imgs_out)\n",
    "    print(f_meta_out)\n",
    "    torch.save(all_imgs, f_imgs_out)\n",
    "    torch.save(df_meta_cell, f_meta_out)\n",
    "\n",
    "\n",
    "get_imgs_from_timelapse_segmentation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14edf160-353a-4b81-959d-421e01e84957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae01c237-b182-4fe7-bb75-f199d1afb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_imgs_out=\"/pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-sep15/imgs-whole-seg-samples.sav\"\n",
    "# f_meta_out=\"/pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-sep15/meta-whole-seg-samples.sav\"\n",
    "# do_save=0\n",
    "# if do_save:\n",
    "#     torch.save(all_imgs, f_imgs_out)\n",
    "#     all_meta.to_csv(f_meta_out)\n",
    "# else: \n",
    "#     all_imgs=torch.load(f_imgs_out)\n",
    "#     df_meta_cell = torch.load(f_meta_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff68c09-b48c-4eb1-9ee4-7f286453ebf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ec2e608-99d1-4db5-8789-a124ce3e8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_imgs_out=os.path.join(DIR_OUT, \"imgs-whole-seg-samples.sav\")\n",
    "f_meta_out=os.path.join(DIR_OUT, \"meta-whole-seg-samples.sav\")\n",
    "# f_meta_out=\"/pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-sep15/meta-whole-seg-samples.sav\"\n",
    "do_save=0\n",
    "if do_save:\n",
    "    torch.save(all_imgs, f_imgs_out)\n",
    "    all_meta.to_csv(f_meta_out)\n",
    "else: \n",
    "    all_imgs=torch.load(f_imgs_out)\n",
    "    df_meta_cell = torch.load(f_meta_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc51e67f-4759-4d55-bb01-07a54a7a76c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 images from 32 unique cells from collection groups ['D0']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(df_meta_cell)} images from {len(df_meta_cell.fname.unique())} unique cells from collection groups {df_meta_cell.g1.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05c81487-cc06-4efb-b4ab-7e4844438932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 things\n",
      "(0, 15.2)    (1, 15.2)    (2, 15.2)    (3, 15.2)    (4, 15.2)    (5, 15.2)    (6, 15.2)    (7, 15.2)    (8, 15.2)    (9, 15.2)    (10, 15.2)    (11, 15.2)    (12, 15.2)    (13, 15.0)    (14, 15.0)    (15, 15.0)    (16, 15.1)    (17, 15.1)    (18, 15.1)    (19, 15.1)    (20, 15.1)    (21, 15.1)    (22, 15.1)    (23, 15.0)    (24, 15.0)    (25, 15.1)    (26, 15.2)    (27, 15.3)    (28, 15.3)    (29, 15.3)    (30, 15.3)    (31, 15.3)    (32, 15.3)    (33, 15.3)    (34, 15.0)    (35, 15.2)    (36, 15.3)    (37, 15.3)    (38, 15.3)    (39, 15.0)    (40, 15.0)    (41, 15.1)    (42, 15.1)    (43, 15.1)    (44, 15.1)    (45, 15.1)    (46, 15.1)    (47, 15.2)    (48, 15.2)    (49, 15.1)    (50, 15.1)    (51, 15.1)    (52, 15.1)    (53, 15.2)    (54, 15.2)    (55, 15.2)    (56, 15.2)    (57, 15.2)    (58, 15.2)    (59, 15.2)    (60, 15.2)    (61, 15.2)    (62, 15.2)    (63, 15.2)    (64, 15.2)    (65, 15.2)    (66, 15.2)    (67, 15.2)    (68, 15.2)    (69, 15.2)    (70, 15.2)    (71, 15.2)    (72, 15.2)    (73, 15.2)    (74, 15.2)    (75, 15.2)    (76, 15.2)    (77, 15.2)    (78, 15.2)    (79, 15.2)    (80, 15.2)    (81, 15.2)    (82, 15.2)    (83, 15.2)    (84, 15.2)    (85, 15.2)    (86, 15.2)    (87, 15.2)    (88, 15.2)    (89, 15.2)    (90, 15.1)    (91, 15.0)    (92, 15.0)    (93, 15.0)    (94, 15.1)    (95, 15.1)    (96, 15.1)    (97, 15.1)    (98, 15.1)    (99, 15.0)    (100, 15.0)    (101, 15.2)    (102, 15.3)    (103, 15.3)    (104, 15.3)    (105, 15.3)    (106, 15.2)    (107, 15.0)    (108, 15.1)    (109, 15.2)    (110, 15.2)    (111, 15.3)    (112, 15.3)    (113, 15.3)    (114, 15.3)    (115, 15.3)    (116, 15.3)    (117, 15.3)    (118, 15.3)    (119, 15.0)    (120, 15.1)    (121, 15.1)    (122, 15.1)    (123, 15.2)    (124, 15.2)    (125, 15.2)    (126, 15.3)    (127, 15.4)    (128, 15.4)    (129, 15.4)    (130, 15.4)    (131, 15.4)    (132, 15.4)    (133, 15.4)    (134, 15.4)    (135, 15.4)    (136, 15.4)    (137, 15.4)    (138, 15.4)    (139, 15.4)    (140, 15.4)    (141, 15.4)    (142, 15.4)    (143, 15.4)    (144, 15.4)    (145, 15.5)    "
     ]
    }
   ],
   "source": [
    "## get the mitochondria - sample pixel size 87, then resize to 64. \n",
    "def extract_mito_from_img_slices(df_subset, all_imgs, channel=1):\n",
    "    \"\"\"\n",
    "    Args are the saved image and dataframe from having run `get_img_slices`. It has \n",
    "    the propery that if df_subset.index==i, then it corresponds to the image \n",
    "    `slice in all_imgs[i]`. \n",
    "    It's therefore fine if `df_subset` is only a subset of the original DataFrame \n",
    "    from the function, as long as the indexing works out. \n",
    "    \n",
    "    Args \n",
    "        df_subset (pd.DataFrame): \n",
    "        channel (int): channel to index the images from `all_imgs`\n",
    "    \"\"\"\n",
    "    all_crops, all_meta = [], [] \n",
    "    \n",
    "    print(len(df_subset), \"things\")\n",
    "    for cnt, (idx, row) in enumerate(df_subset.iterrows()):\n",
    "        mem_pcnt = psutil.virtual_memory().percent\n",
    "        print(f\"({cnt}, {mem_pcnt})   \", end=\" \")\n",
    "        img = all_imgs[idx]\n",
    "        # print(idx, img.sum().item(), \"    \", end=\"\")\n",
    "        cellid = row.cellid\n",
    "        cellid_ts = row.cellid_ts\n",
    "\n",
    "        img_labels = measure.label(img[channel], connectivity=1)\n",
    "        uniq_labels = np.unique(img_labels)[1:]\n",
    "\n",
    "        prop_keys = ['centroid', \"area\", 'eccentricity', 'convex_area', 'equivalent_diameter','extent','local_centroid' ]\n",
    "        columns = ['cellid_ts'] + prop_keys + [\"this_index\"]\n",
    "\n",
    "        for y, l in enumerate(uniq_labels): \n",
    "            # mask and regions \n",
    "            mask = np.zeros_like(img_labels)\n",
    "            mask[img_labels==l]=1\n",
    "\n",
    "            # extract the basic properties \n",
    "            r = measure.regionprops(mask)[0]\n",
    "            meta = [cellid_ts] + [r[p] for p in prop_keys] + [l]\n",
    "            l0, l1, u0, u1 = r.bbox\n",
    "\n",
    "            # do image crop\n",
    "            slc0, slc1 = slice(l0, u0), slice(l1,u1)\n",
    "            crop = mask[slc0, slc1]\n",
    "            crop = bad.center_img(crop[None], dims=2, by_channel=0)\n",
    "\n",
    "            all_crops.append(crop)\n",
    "            all_meta.append(meta)\n",
    "\n",
    "    all_meta = pd.DataFrame(data=all_meta, columns=columns)\n",
    "\n",
    "    return all_meta, all_crops\n",
    "\n",
    "# do the data stuff over 2 datasets. Why did I do it over 2 dataset? Probably because of memory problems. \n",
    "import torchvision.transforms.functional as TF\n",
    "data_dir_out = DIR_OUT\n",
    "\n",
    "df_subset = df_meta_cell[:400]\n",
    "df_meta_mito, all_crops = extract_mito_from_img_slices(df_subset, all_imgs)\n",
    "data, _ = bad.put_centered_imgs_to_standard_sz(all_crops, None, sz=87, dims=2, by_channel=0,keep_too_big_cells=1)\n",
    "data = TF.resize(torch.Tensor(data), 64)\n",
    "f_out_mito0 = os.path.join(data_dir_out, \"mito0.sav\")\n",
    "torch.save([data, df_meta_cell, df_meta_mito,], f_out_mito0)\n",
    "\n",
    "# del df_meta_mito, all_crops\n",
    "# df_subset = df_meta_cell[400:]\n",
    "# df_meta_mito, all_crops = extract_mito_from_img_slices(df_subset, all_imgs)\n",
    "# data, _ = bad.put_centered_imgs_to_standard_sz(all_crops, None, sz=87, dims=2, by_channel=0,keep_too_big_cells=1)\n",
    "# data = TF.resize(torch.Tensor(data), 64)\n",
    "# f_out_mito1 = os.path.join(data_dir_out, \"mito1.sav\")\n",
    "# torch.save([data, df_meta_cell, df_meta_mito,], f_out_mito1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d91546-f404-4a9b-b36d-d4b652e6eb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec51b3f9-f285-4fc5-960f-a76597ea9b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-oct26-D0-only/mito-data.sav\n",
      "/pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-oct26-D0-only/mito-meta-cell.sav\n",
      "/pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-oct26-D0-only/mito-meta-mito.sav\n",
      "/pasteur/u/jmhb/confocal-2d-video-processing/analyses/jan22-neural-diff/data-oct26-D0-only/imgs-whole-seg-samples.sav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "# get data \n",
    "data_dir_out = DIR_OUT\n",
    "f_out_mito0 = os.path.join(data_dir_out, \"mito0.sav\")\n",
    "# f_out_mito1 = os.path.join(data_dir_out, \"mito1.sav\")\n",
    "[data0, df_meta_cell0, df_meta_mito0] = torch.load(f_out_mito0)\n",
    "# [data1, df_meta_cell1, df_meta_mito1] = torch.load(f_out_mito1)\n",
    "# joining \n",
    "data = torch.cat([data0])\n",
    "df_meta_mito = pd.concat([df_meta_mito0,]).reset_index()\n",
    "# df_meta_cell = df_meta_cell1 # they're the same\n",
    "# save everything \n",
    "f_out = os.path.join(data_dir_out, \"mito-data.sav\")\n",
    "torch.save(data, f_out)\n",
    "print(f_out)\n",
    "f_out = os.path.join(data_dir_out, \"mito-meta-cell.sav\")\n",
    "torch.save(df_meta_cell, f_out)\n",
    "print(f_out)\n",
    "f_out = os.path.join(data_dir_out, \"mito-meta-mito.sav\")\n",
    "torch.save(df_meta_mito, f_out)\n",
    "print(f_out)\n",
    "f_out = os.path.join(data_dir_out, \"imgs-whole-seg-samples.sav\")\n",
    "torch.save(all_imgs, f_out)\n",
    "print(f_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
